{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPAHTj893IKgSSO55JvFyLy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1xyNNgShaeCEKoqiOFZVjcGtYks1CyE_Y"},"id":"piFz6BU0Wx6d","executionInfo":{"status":"ok","timestamp":1763145573804,"user_tz":-330,"elapsed":4459736,"user":{"displayName":"Vijay Chamyal","userId":"01306763949782686858"}},"outputId":"b5d9e55b-1687-40c7-b39e-c2056668ac34"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\n","!pip install torch torchaudio torchvision\n","\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils import spectral_norm\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from IPython.display import Audio, display\n","import copy\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 1. DATASET WITH NORMALIZATION\n","\n","class ImprovedAudioDataset(Dataset):\n","    def __init__(self, root_dir, categories, max_frames=512, fraction=1.0):\n","        self.root_dir = root_dir\n","        self.categories = categories\n","        self.max_frames = max_frames\n","        self.file_list = []\n","        self.class_to_idx = {cat: i for i, cat in enumerate(categories)}\n","\n","        for cat_name in self.categories:\n","            cat_dir = os.path.join(root_dir, cat_name)\n","            files_in_cat = [os.path.join(cat_dir, f) for f in os.listdir(cat_dir) if f.endswith(\".wav\")]\n","            num_to_sample = int(len(files_in_cat) * fraction)\n","            sampled_files = random.sample(files_in_cat, num_to_sample)\n","            label_idx = self.class_to_idx[cat_name]\n","            self.file_list.extend([(file_path, label_idx) for file_path in sampled_files])\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        path, label = self.file_list[idx]\n","        wav, sr = torchaudio.load(path)\n","        if wav.size(0) > 1:\n","            wav = wav.mean(dim=0, keepdim=True)\n","\n","        mel_spec = torchaudio.transforms.MelSpectrogram(\n","            sample_rate=sr, n_fft=1024, hop_length=256, n_mels=128\n","        )(wav)\n","        log_spec = torch.log1p(mel_spec)\n","\n","        # Normalize to [-1, 1] for better training\n","        log_spec = (log_spec - log_spec.mean()) / (log_spec.std() + 1e-8)\n","\n","        _, _, n_frames = log_spec.shape\n","        if n_frames < self.max_frames:\n","            pad = self.max_frames - n_frames\n","            log_spec = F.pad(log_spec, (0, pad))\n","        else:\n","            log_spec = log_spec[:, :, :self.max_frames]\n","\n","        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n","        return log_spec, label_vec\n","\n","# 2. IMPROVED GENERATOR\n","\n","class ImprovedGenerator(nn.Module):\n","    def __init__(self, latent_dim, num_categories, spec_shape=(128, 512)):\n","        super().__init__()\n","        self.latent_dim = latent_dim\n","        self.num_categories = num_categories\n","        self.spec_shape = spec_shape\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(latent_dim + num_categories, 256 * 8 * 32),\n","            nn.BatchNorm1d(256 * 8 * 32),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.unflatten_shape = (256, 8, 32)\n","\n","        self.net = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 16x64\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 32x128\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # 64x256\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","\n","            nn.ConvTranspose2d(32, 1, 4, 2, 1),     # 128x512\n","            nn.Tanh()  # Output in [-1, 1] to match normalized input\n","        )\n","\n","    def forward(self, z, y):\n","        h = torch.cat([z, y], dim=1)\n","        h = self.fc(h)\n","        h = h.view(-1, *self.unflatten_shape)\n","        return self.net(h)\n","\n","# 3. IMPROVED DISCRIMINATOR WITH SPECTRAL NORMALIZATION\n","\n","\n","class ImprovedDiscriminator(nn.Module):\n","    def __init__(self, num_categories, spec_shape=(128, 512)):\n","        super().__init__()\n","        self.num_categories = num_categories\n","        self.spec_shape = spec_shape\n","        H, W = spec_shape\n","\n","        self.label_embedding = nn.Linear(num_categories, H * W)\n","\n","        # Spectral normalization for Lipschitz constraint\n","        self.conv1 = spectral_norm(nn.Conv2d(2, 64, 4, 2, 1))\n","        self.conv2 = spectral_norm(nn.Conv2d(64, 128, 4, 2, 1))\n","        self.conv3 = spectral_norm(nn.Conv2d(128, 256, 4, 2, 1))\n","        self.conv4 = spectral_norm(nn.Conv2d(256, 512, 4, 2, 1))\n","        self.conv5 = spectral_norm(nn.Conv2d(512, 1, (8, 32), 1, 0))\n","\n","    def forward(self, spec, y):\n","        label_map = self.label_embedding(y).view(-1, 1, *self.spec_shape)\n","        h = torch.cat([spec, label_map], dim=1)\n","\n","        h = F.leaky_relu(self.conv1(h), 0.2)\n","        h = F.leaky_relu(self.conv2(h), 0.2)\n","        h = F.leaky_relu(self.conv3(h), 0.2)\n","        h = F.leaky_relu(self.conv4(h), 0.2)\n","        logit = self.conv5(h)\n","\n","        return logit.view(-1, 1)\n","\n","# 4. WGAN-GP: GRADIENT PENALTY\n","\n","def compute_gradient_penalty(discriminator, real_samples, fake_samples, labels, device):\n","    batch_size = real_samples.size(0)\n","    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n","    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n","\n","    d_interpolates = discriminator(interpolates, labels)\n","    gradients = torch.autograd.grad(\n","        outputs=d_interpolates,\n","        inputs=interpolates,\n","        grad_outputs=torch.ones_like(d_interpolates),\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True\n","    )[0]\n","\n","    gradients = gradients.view(batch_size, -1)\n","    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","    return gradient_penalty\n","\n","# 5. EXPONENTIAL MOVING AVERAGE FOR GENERATOR\n","\n","def update_ema(ema_model, model, decay=0.999):\n","    with torch.no_grad():\n","        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n","            ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n","\n","# 6. CHECKPOINT MANAGEMENT\n","\n","def save_checkpoint(gen, disc, opt_g, opt_d, epoch, path):\n","    torch.save({\n","        'epoch': epoch,\n","        'gen_state': gen.state_dict(),\n","        'disc_state': disc.state_dict(),\n","        'opt_g_state': opt_g.state_dict(),\n","        'opt_d_state': opt_d.state_dict()\n","    }, path)\n","    print(f\"âœ“ Checkpoint saved: {path}\")\n","\n","def load_checkpoint(gen, disc, opt_g, opt_d, path, device):\n","    checkpoint = torch.load(path, map_location=device)\n","    gen.load_state_dict(checkpoint['gen_state'])\n","    disc.load_state_dict(checkpoint['disc_state'])\n","    opt_g.load_state_dict(checkpoint['opt_g_state'])\n","    opt_d.load_state_dict(checkpoint['opt_d_state'])\n","    print(f\"âœ“ Loaded checkpoint from epoch {checkpoint['epoch']}\")\n","    return checkpoint['epoch']\n","\n","# 7. IMPROVED AUDIO GENERATION\n","\n","def generate_audio(generator, category_idx, num_samples, device, categories,\n","                   mean=0, std=1, sample_rate=22050):\n","    generator.eval()\n","    y = F.one_hot(torch.tensor([category_idx]), num_classes=len(categories)).float().to(device)\n","    z = torch.randn(num_samples, generator.latent_dim, device=device)\n","\n","    with torch.no_grad():\n","        log_spec_gen = generator(z, y)\n","\n","    # Denormalize\n","    log_spec_gen = log_spec_gen * std + mean\n","    spec_gen = torch.expm1(log_spec_gen).squeeze(1)\n","\n","    inverse_mel = torchaudio.transforms.InverseMelScale(\n","        n_stft=513, n_mels=128, sample_rate=sample_rate\n","    ).to(device)\n","    linear_spec = inverse_mel(spec_gen)\n","\n","    # More Griffin-Lim iterations for better quality\n","    griffin = torchaudio.transforms.GriffinLim(\n","        n_fft=1024, hop_length=256, win_length=1024, n_iter=64\n","    ).to(device)\n","\n","    waveform = griffin(linear_spec)\n","    return waveform.cpu()\n","\n","# 8. TRAINING FUNCTION WITH WGAN-GP\n","\n","\n","def train_improved_gan(generator, discriminator, dataloader, device, categories,\n","                       epochs, lr_g=1e-4, lr_d=4e-4, latent_dim=100,\n","                       lambda_gp=10, n_critic=5, ema_decay=0.999,\n","                       resume_path=None):\n","\n","    # Separate learning rates (TTUR: Two Time-scale Update Rule)\n","    opt_g = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(0.0, 0.9))\n","    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.0, 0.9))\n","\n","    # EMA generator for better samples\n","    ema_generator = copy.deepcopy(generator).to(device)\n","\n","    # Create directories\n","    os.makedirs(\"improved_audio\", exist_ok=True)\n","    os.makedirs(\"improved_plots\", exist_ok=True)\n","    os.makedirs(\"checkpoints\", exist_ok=True)\n","\n","    start_epoch = 1\n","    if resume_path and os.path.exists(resume_path):\n","        start_epoch = load_checkpoint(generator, discriminator, opt_g, opt_d, resume_path, device) + 1\n","\n","    # Compute dataset statistics for denormalization\n","    all_specs = []\n","    for batch_specs, _ in dataloader:\n","        all_specs.append(batch_specs)\n","        if len(all_specs) >= 10:  # Sample from first 10 batches\n","            break\n","    all_specs = torch.cat(all_specs, dim=0)\n","    data_mean = all_specs.mean().item()\n","    data_std = all_specs.std().item()\n","\n","    for epoch in range(start_epoch, epochs + 1):\n","        generator.train()\n","        discriminator.train()\n","\n","        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n","        epoch_d_loss = 0\n","        epoch_g_loss = 0\n","\n","        for batch_idx, (real_specs, labels) in enumerate(loop):\n","            real_specs = real_specs.to(device)\n","            labels = labels.to(device)\n","            batch_size = real_specs.size(0)\n","\n","            # Train Discriminator (n_critic times)\n","            for _ in range(n_critic):\n","                opt_d.zero_grad()\n","\n","                real_validity = discriminator(real_specs, labels)\n","\n","                z = torch.randn(batch_size, latent_dim, device=device)\n","                fake_specs = generator(z, labels)\n","                fake_validity = discriminator(fake_specs.detach(), labels)\n","\n","                gp = compute_gradient_penalty(discriminator, real_specs, fake_specs, labels, device)\n","                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n","\n","                d_loss.backward()\n","                opt_d.step()\n","\n","            # Train Generator\n","            opt_g.zero_grad()\n","\n","            z = torch.randn(batch_size, latent_dim, device=device)\n","            fake_specs = generator(z, labels)\n","            fake_validity = discriminator(fake_specs, labels)\n","            g_loss = -torch.mean(fake_validity)\n","\n","            g_loss.backward()\n","            opt_g.step()\n","\n","            # Update EMA generator\n","            update_ema(ema_generator, generator, ema_decay)\n","\n","            epoch_d_loss += d_loss.item()\n","            epoch_g_loss += g_loss.item()\n","\n","            loop.set_postfix(D_loss=d_loss.item(), G_loss=g_loss.item(), GP=gp.item())\n","\n","        avg_d_loss = epoch_d_loss / len(dataloader)\n","        avg_g_loss = epoch_g_loss / len(dataloader)\n","        print(f\"\\nðŸ“Š Epoch {epoch}: D_Loss={avg_d_loss:.4f}, G_Loss={avg_g_loss:.4f}\")\n","\n","        # Generate samples every 10 epochs\n","        if epoch % 10 == 0 or epoch == 1:\n","            print(f\"\\nðŸŽ¨ Generating samples (Epoch {epoch})...\")\n","            ema_generator.eval()\n","\n","            fig, axes = plt.subplots(1, len(categories), figsize=(4 * len(categories), 4))\n","            if len(categories) == 1:\n","                axes = [axes]\n","\n","            for cat_idx, cat_name in enumerate(categories):\n","                y_cond = F.one_hot(torch.tensor([cat_idx]), num_classes=len(categories)).float().to(device)\n","                z_sample = torch.randn(1, latent_dim).to(device)\n","\n","                with torch.no_grad():\n","                    spec_gen = ema_generator(z_sample, y_cond)\n","\n","                spec_np = spec_gen.squeeze().cpu().numpy()\n","                axes[cat_idx].imshow(spec_np, aspect='auto', origin='lower', cmap='viridis')\n","                axes[cat_idx].set_title(f'{cat_name}\\n(Epoch {epoch})')\n","                axes[cat_idx].axis('off')\n","\n","            plt.tight_layout()\n","            plt.savefig(f'improved_plots/epoch_{epoch:03d}.png')\n","            plt.show()\n","            plt.close(fig)\n","\n","            # Generate audio\n","            for cat_idx, cat_name in enumerate(categories):\n","                wav = generate_audio(ema_generator, cat_idx, 1, device, categories,\n","                                    data_mean, data_std)\n","                fname = f\"improved_audio/{cat_name}_ep{epoch:03d}.wav\"\n","                torchaudio.save(fname, wav, sample_rate=22050)\n","                print(f\"ðŸ’¾ Saved: {fname}\")\n","                display(Audio(data=wav.numpy(), rate=22050))\n","\n","        # Save checkpoint every 25 epochs\n","        if epoch % 25 == 0:\n","            save_checkpoint(generator, discriminator, opt_g, opt_d, epoch,\n","                          f'checkpoints/checkpoint_epoch_{epoch:03d}.pth')\n","\n","# 9. MAIN EXECUTION\n","\n","if __name__ == '__main__':\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    LATENT_DIM = 128\n","    EPOCHS = 200\n","    BATCH_SIZE = 32\n","    LR_G = 1e-4  # Generator learning rate\n","    LR_D = 4e-4  # Discriminator learning rate (4x generator for WGAN-GP)\n","    LAMBDA_GP = 10\n","    N_CRITIC = 5\n","    EMA_DECAY = 0.999\n","\n","    BASE_PATH = 'drive/MyDrive/organized_dataset/'\n","    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n","    train_categories = sorted([d for d in os.listdir(TRAIN_PATH)\n","                              if os.path.isdir(os.path.join(TRAIN_PATH, d))])\n","\n","    print(f\" Device: {DEVICE}\")\n","    print(f\" Categories ({len(train_categories)}): {train_categories}\")\n","\n","    train_dataset = ImprovedAudioDataset(TRAIN_PATH, train_categories)\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                             shuffle=True, num_workers=2, pin_memory=True)\n","\n","    generator = ImprovedGenerator(LATENT_DIM, len(train_categories)).to(DEVICE)\n","    discriminator = ImprovedDiscriminator(len(train_categories)).to(DEVICE)\n","\n","    print(f\"\\n Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n","    print(f\"Discriminator params: {sum(p.numel() for p in discriminator.parameters()):,}\")\n","\n","    train_improved_gan(\n","        generator=generator,\n","        discriminator=discriminator,\n","        dataloader=train_loader,\n","        device=DEVICE,\n","        categories=train_categories,\n","        epochs=EPOCHS,\n","        lr_g=LR_G,\n","        lr_d=LR_D,\n","        latent_dim=LATENT_DIM,\n","        lambda_gp=LAMBDA_GP,\n","        n_critic=N_CRITIC,\n","        ema_decay=EMA_DECAY,\n","        resume_path=None  # Set to checkpoint path to resume\n","    )\n","\n","    print(\"\\nTraining complete!\")\n"]}]}